{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "171c0f89-43ed-48ab-8e83-fe2332f33e4b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b633e8a-ab70-4df7-bcda-ecf325ecda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "import torch.autograd.profiler as profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b20107-e049-4761-b3f0-51ecde65347c",
   "metadata": {},
   "source": [
    "We need to measure three things potentially 4\n",
    "- Model Size\n",
    "- Inference\n",
    "- Memory Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c2273-7bf0-4932-a76c-33206bd1c3f2",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3061b9ce-cf97-4f6d-a9f9-e8a8baea39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import shufflenet_v2_x1_0\n",
    "shufflenet = shufflenet_v2_x1_0(pretrained=True)\n",
    "shufflenet = model = torch.load(\"./ShuffleNetModels/shuffle_trained_Deepfakes.pth\", weights_only=False, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db60ea0e-e2a3-45e4-ba63-321266ae298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet = timm.create_model('mobilenetv2_100', pretrained=True, num_classes=1)\n",
    "mobilenet = torch.load(\"./MobileNetModels/mobilenet_trained_Deepfakes.pth\", weights_only=False, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4ae6f3c-7ec2-47af-ac23-a669fd59a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghostnet = torch.load(\"./GhostNetModels/ghostnet_trained_Deepfakes.pth\", weights_only=False, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce3bb835-2c31-4ad8-b4a1-15fbed9da559",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet = torch.load(\"./EfficientNetLiteModels/efficientnet_trained_Deepfakes.pth\", weights_only=False, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3047a24c-a47f-480e-a09d-06857cced093",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinyvit = torch.load(\"./TinyVitModels/tinyvit_trained_Deepfakes.pth\", weights_only=False, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dbc04fe9-0034-451b-abb8-9825cce93ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilevit = torch.load(\"./MobileViTModels/mobilevit_trained_Deepfakes.pth\", weights_only=False, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a12ad322-0f2c-47cc-9db4-73778713f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "convnext = torch.load(\"./ConvNeXtModels/convnext_trained_FaceShifter.pth\", weights_only=False, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f6ffe-b2fc-4db7-85ac-d17e69246d29",
   "metadata": {},
   "source": [
    "# Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7917b36d-a2eb-4310-97f6-81848cd52b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    \"\"\"\n",
    "    Returns the size of a PyTorch model in megabytes.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "        float: The size of the model in megabytes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Save the model to a temporary file\n",
    "    torch.save(model.state_dict(), \"temp_model.pth\")\n",
    "\n",
    "    # Get the file size in bytes\n",
    "    size_bytes = os.path.getsize(\"temp_model.pth\")\n",
    "\n",
    "    # Delete the temporary file\n",
    "    os.remove(\"temp_model.pth\")\n",
    "\n",
    "    # Convert bytes to megabytes\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    print(f\"Model size: {size_mb:.2f} MB\")\n",
    "    return size_mb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f54b86f-4ffc-4ef1-9505-5ab42d9075b1",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6130d677-9eec-4463-abca-e57b35ddbe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "def measure_inference_time(model, frame_path, num_iterations=1000, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Measures the average inference time of a model on a single input frame.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The deep learning model to evaluate.\n",
    "        frame_path (str): Path to the image file used for inference.\n",
    "        num_iterations (int, optional): Number of times to repeat inference for accurate timing. Default is 1000.\n",
    "        device (str, optional): Device to run inference on (\"cpu\" or \"cuda\"). Default is \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average inference time in milliseconds, standard deviation in milliseconds)\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image from the given file path and convert it to a PyTorch tensor\n",
    "    # - `cv2.imread()` loads the image as a NumPy array (H x W x C, BGR format)\n",
    "    # - `torch.from_numpy()` converts it to a PyTorch tensor\n",
    "    # - `.permute(2,0,1)` changes the shape to (C x H x W) for PyTorch compatibility\n",
    "    # - `.float()` ensures it's a floating-point tensor\n",
    "    frame = torch.from_numpy(cv2.imread(frame_path)).permute(2,0,1).float()\n",
    "\n",
    "    # Move model to the specified device (CPU or GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode (disables dropout, batch norm updates)\n",
    "    model.eval()\n",
    "\n",
    "    # Add a batch dimension to the input tensor (B x C x H x W) if needed\n",
    "    frame = frame.unsqueeze(0)  \n",
    "\n",
    "    # Move input frame to the specified device\n",
    "    frame = frame.to(device)\n",
    "\n",
    "    # List to store inference times for each iteration\n",
    "    inference_times = []\n",
    "\n",
    "    # Disable gradient computation for efficiency during inference\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            start_time = time.time()  # Record start time\n",
    "            _ = model(frame)  # Run inference (output ignored)\n",
    "            end_time = time.time()  # Record end time\n",
    "\n",
    "            # Compute and store inference time for this iteration\n",
    "            inference_times.append(end_time - start_time)\n",
    "\n",
    "    # Compute the average inference time in milliseconds\n",
    "    avg_inference_time = sum(inference_times) / len(inference_times) * 1000\n",
    "\n",
    "    # Compute the standard deviation of inference time (converted to milliseconds)\n",
    "    std_inference_time = torch.std(torch.tensor(inference_times)).item() * 1000\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Average inference time: {avg_inference_time:.2f} milliseconds\")\n",
    "    print(f\"Standard deviation: {std_inference_time:.2f} milliseconds\")\n",
    "\n",
    "    # Return the average inference time and standard deviation in milliseconds\n",
    "    return avg_inference_time, std_inference_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abbe96f-fc12-4a1e-b680-40a39298cc32",
   "metadata": {},
   "source": [
    "# Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3823bf5a-fc97-45d8-8d81-f7e532c0d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_cpu_memory(model, input_shape=(1, 3, 224, 224)):\n",
    "    \"\"\"\n",
    "    Measures peak CPU memory usage during inference using PyTorch Profiler.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to profile.\n",
    "        input_shape (tuple): Shape of the input tensor.\n",
    "\n",
    "    Returns:\n",
    "        float: Peak memory allocated (in MB)\n",
    "    \"\"\"\n",
    "    device = \"cpu\"  # Ensure model runs on CPU\n",
    "\n",
    "    # Move model to CPU\n",
    "    model.to(device)\n",
    "    model.eval()  # Ensure model is in evaluation mode\n",
    "\n",
    "    # Create a dummy input tensor on CPU\n",
    "    input_tensor = torch.randn(input_shape).to(device)\n",
    "\n",
    "    # Profile CPU memory usage\n",
    "    with torch.no_grad():\n",
    "        with profiler.profile(profile_memory=True) as prof:\n",
    "            model(input_tensor)\n",
    "\n",
    "    # Extract peak CPU memory allocated\n",
    "    peak_memory_mb = max(event.self_cpu_memory_usage for event in prof.key_averages()) / 1024**2\n",
    "\n",
    "    print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n",
    "    print(f\"\\nðŸ”¹ CPU Peak Memory Allocated: {peak_memory_mb:.2f} MB\")\n",
    "\n",
    "    return peak_memory_mb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4625c9e-e4cc-45b5-8e41-37c1a7fc7d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                 aten::addmm        65.83%     140.040ms        68.28%     145.260ms       3.926ms      40.91 Mb      40.91 Mb            37  \n",
      "                  aten::gelu         8.44%      17.955ms         8.44%      17.955ms     997.491us      32.73 Mb      32.73 Mb            18  \n",
      "                 aten::empty         0.46%     983.540us         0.46%     983.540us       8.265us      26.81 Mb      26.81 Mb           119  \n",
      "                   aten::add         2.75%       5.844ms         2.77%       5.899ms     226.893us      11.37 Mb      11.37 Mb            26  \n",
      "                   aten::mul         1.16%       2.461ms         1.16%       2.461ms     111.860us      11.34 Mb      11.34 Mb            22  \n",
      "                   aten::sub         0.39%     822.669us         0.39%     822.669us     102.834us       6.32 Mb       6.32 Mb             8  \n",
      "                   aten::pow         0.20%     416.444us         0.20%     422.395us     105.599us       3.16 Mb       3.16 Mb             4  \n",
      "                   aten::div         0.13%     278.282us         0.13%     278.282us      69.570us       3.16 Mb       3.16 Mb             4  \n",
      "                  aten::mean         0.08%     164.773us         0.56%       1.202ms     133.511us      59.66 Kb      59.66 Kb             9  \n",
      "                  aten::sqrt         0.04%      77.676us         0.04%      77.676us      19.419us      28.33 Kb      28.33 Kb             4  \n",
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 212.729ms\n",
      "\n",
      "\n",
      "ðŸ”¹ CPU Peak Memory Allocated: 40.91 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40.913089752197266"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_cpu_memory(convnext, input_shape=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c6e71-97da-42c1-a7bd-870cbbc62e16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
